{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Project 3\n",
        "\n",
        "Avik Bhattacharya\n",
        "\n",
        "Some of the comments are similar to the comments of Project 2."
      ],
      "metadata": {
        "id": "ahps0DcgB69y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Imports/Setups"
      ],
      "metadata": {
        "id": "rcIgi1KkCVyV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Graphical libraries\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "mpl.rcParams['figure.dpi'] = 120\n",
        "from IPython.display import Image\n",
        "from IPython.display import display\n",
        "plt.style.use('seaborn-white')"
      ],
      "metadata": {
        "id": "X7fyberWB77W"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --q scipy"
      ],
      "metadata": {
        "id": "aZTkQkZFKBIQ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression, Ridge\n",
        "from sklearn.preprocessing import StandardScaler, QuantileTransformer, MinMaxScaler, PolynomialFeatures\n",
        "from sklearn.decomposition import PCA\n",
        "from scipy.spatial import Delaunay\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.pipeline import Pipeline\n",
        "import scipy.stats as stats \n",
        "from sklearn.model_selection import train_test_split as tts, KFold, GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error as mse\n",
        "from scipy.interpolate import interp1d, RegularGridInterpolator, griddata, LinearNDInterpolator, NearestNDInterpolator\n",
        "from math import ceil\n",
        "from scipy import linalg\n",
        "\n",
        "#Used in scikit compliant functions\n",
        "from sklearn.base import BaseEstimator, RegressorMixin\n",
        "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted"
      ],
      "metadata": {
        "id": "EkcqnFEdKKad"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Will be used later.\n",
        "scale = StandardScaler()\n",
        "\n",
        "#Will be used in Question 3.\n",
        "lm = LinearRegression()"
      ],
      "metadata": {
        "id": "fB3kaWxfK2Vj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Datasets used\n",
        "cars = pd.read_csv('drive/MyDrive/ADV Applied Machine Learning/Module 1/Day 3/cars.csv')\n",
        "concrete = pd.read_csv('drive/MyDrive/ADV Applied Machine Learning/Module 1/Day 3/concrete.csv')\n",
        "housing = pd.read_csv('drive/MyDrive/ADV Applied Machine Learning/Homework/housing.csv')"
      ],
      "metadata": {
        "id": "Z1E-1iwxLydB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Euclidean distance between all the observations in u and v.\n",
        "#Used in the lowess\n",
        "def dist(u,v):\n",
        "  if len(v.shape)==1: #If v is one dimensional, it is forced into a row vector.\n",
        "    v = v.reshape(1,-1)\n",
        "  d = np.array([np.sqrt(np.sum((u-v[i])**2,axis=1)) for i in range(len(v))])\n",
        "  return d"
      ],
      "metadata": {
        "id": "uAXjfTcrTTDU"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Lowess from class with an input to pick the kernel for Question 2\n",
        "def lw_ag_md(x, y, xnew,f=2/3,iter=3, intercept=True, kernel='Tricubic'):\n",
        "\n",
        "  n = len(x) #Number of observations\n",
        "  r = int(ceil(f * n)) #Calculating the size of the neighborhood\n",
        "  yest = np.zeros(n)\n",
        "\n",
        "  if len(y.shape)==1: #Here we make column vector\n",
        "    y = y.reshape(-1,1)\n",
        "\n",
        "  if len(x.shape)==1: #Also making column vectors.\n",
        "    x = x.reshape(-1,1)\n",
        "  \n",
        "  if intercept:\n",
        "    x1 = np.column_stack([np.ones((len(x),1)),x])\n",
        "  else:\n",
        "    x1 = x\n",
        "\n",
        "  #Finds the difference between each value of x, sorts them, and only gives you the values in the neighberhood.\n",
        "  h = [np.sort(np.sqrt(np.sum((x-x[i])**2,axis=1)))[r] for i in range(n)]\n",
        "  \n",
        "  #Calculates the Euclidean distance and makes sure the weights don't go above 1 or under 0.\n",
        "  w = np.clip(dist(x,x) / np.array(h), 0.0, 1.0)\n",
        "  \n",
        "  #Creates the weight using a kernel of your choice. Default is tricubic.\n",
        "  #Kernels are defined later on in Question 2.\n",
        "  if(kernel=='Tricubic'):\n",
        "    w = Tricubic(w)\n",
        "  elif(kernel=='Epanechnikov'):\n",
        "    w = Epanechnikov(w)\n",
        "  elif(kernel=='Quartic'):\n",
        "    w = Quartic(w)\n",
        "  elif(kernel=='Gaussian'):\n",
        "    w = Gaussian(w)\n",
        "  else:\n",
        "    w = Tricubic(w)\n",
        "\n",
        "  #Looping through all X-points\n",
        "  delta = np.ones(n)\n",
        "  for iteration in range(iter):\n",
        "    for i in range(n):\n",
        "      #The multiplication of two diagonal matrices will create another diagonal matrix.\n",
        "      W = np.diag(delta).dot(np.diag(w[i,:]))\n",
        "      b = np.transpose(x1).dot(W).dot(y)\n",
        "      A = np.transpose(x1).dot(W).dot(x1)\n",
        "      ##\n",
        "      A = A + 0.0001*np.eye(x1.shape[1]) #If we want L2 regularization for solving the system\n",
        "      beta = linalg.solve(A, b)\n",
        "      #beta, res, rnk, s = linalg.lstsq(A, b)\n",
        "      yest[i] = np.dot(x1[i],beta.ravel())\n",
        "      #The .ravel() method used above and below returns a flattened array.\n",
        "\n",
        "    residuals = y.ravel() - yest\n",
        "    s = np.median(np.abs(residuals))\n",
        "    delta = np.clip(residuals / (6.0 * s), -1, 1) #Clips the very high and low outliers.\n",
        "    delta = (1 - delta ** 2) ** 2 #Very low residuals get centered at a weight of 1.\n",
        "    \n",
        "  #Here we are making predictions for xnew by using an interpolation and the predictions we made for the train data\n",
        "  if x.shape[1]==1:\n",
        "    f = interp1d(x.flatten(),yest,fill_value='extrapolate')\n",
        "    output = f(xnew)\n",
        "  else:\n",
        "    output = np.zeros(len(xnew))\n",
        "    for i in range(len(xnew)):\n",
        "      ind = np.argsort(np.sqrt(np.sum((x-xnew[i])**2,axis=1)))[:r]\n",
        "      #Has Delauney triangulation work\n",
        "      #Also prevents code from running too long.\n",
        "      pca = PCA(n_components=3)\n",
        "      x_pca = pca.fit_transform(x[ind])\n",
        "      tri = Delaunay(x_pca,qhull_options='QJ Pp')\n",
        "      f = LinearNDInterpolator(tri,yest[ind])\n",
        "      output[i] = f(pca.transform(xnew[i].reshape(1,-1))) \n",
        "      #The output may have NaN's where the data points from xnew are outside the convex hull of X\n",
        "\n",
        "  if sum(np.isnan(output))>0:\n",
        "    g = NearestNDInterpolator(x,yest.ravel()) \n",
        "    # output[np.isnan(output)] = g(X[np.isnan(output)])\n",
        "    output[np.isnan(output)] = g(xnew[np.isnan(output)])\n",
        "  return output"
      ],
      "metadata": {
        "id": "xCOYl0HpT8SZ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#From Project 2 instead of the class example because it has the kernel variable included.\n",
        "class Lowess_AG_MD:\n",
        "    def __init__(self, f = 1/10, iter = 3, intercept=True, kernel='Tricubic'):\n",
        "        self.f = f\n",
        "        self.iter = iter\n",
        "        self.intercept = intercept\n",
        "        self.kernel= kernel\n",
        "    \n",
        "    def fit(self, x, y):\n",
        "        f = self.f\n",
        "        iter = self.iter\n",
        "        kernel = self.kernel\n",
        "        self.xtrain_ = x\n",
        "        self.yhat_ = y\n",
        "\n",
        "    def predict(self, x_new):\n",
        "        check_is_fitted(self)\n",
        "        x = self.xtrain_\n",
        "        y = self.yhat_\n",
        "        f = self.f\n",
        "        iter = self.iter\n",
        "        intercept = self.intercept\n",
        "        kernel = self.kernel\n",
        "        return lw_ag_md(x, y, x_new, f, iter, intercept, kernel) #Version of lowess from above.\n",
        "\n",
        "    def get_params(self, deep=True):\n",
        "    #Suppose this estimator has parameters \"f\", \"iter\" , \"intercept, and \"kernel\"\n",
        "        return {\"f\": self.f, \"iter\": self.iter,\"intercept\":self.intercept,\"kernel\":self.kernel}\n",
        "\n",
        "    def set_params(self, **parameters):\n",
        "        for parameter, value in parameters.items():\n",
        "            setattr(self, parameter, value)\n",
        "        return self"
      ],
      "metadata": {
        "id": "3JvBqn7IZE8l"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Question 1"
      ],
      "metadata": {
        "id": "JWAX83sJCX2O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement the Gradient Boosting algorithm with user defined choices for Regressor_1 and Regressor_2"
      ],
      "metadata": {
        "id": "he41jFRMCohm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Added f1/f2, iter1/iter2, and kernel1/kernel2 to allow user defined regressors and allow the regressors to be different. \n",
        "def boosted_lwr(x, y, xnew, f1=1/3, f2=1/3, iter1=2, iter2=2, intercept=True, kernel1='Epanechnikov', kernel2='Epanechnikov'):\n",
        "  #We need decision trees\n",
        "  #For training the boosted method we use x and y\n",
        "  model1 = Lowess_AG_MD(f=f1, iter=iter1, kernel=kernel1) #We need this for training the Decision Tree\n",
        "  model1.fit(x,y)\n",
        "  residuals1 = y - model1.predict(x)\n",
        "  model2 = Lowess_AG_MD(f=f2, iter=iter2, kernel=kernel2)\n",
        "  #model2 = RandomForestRegressor(n_estimators=200,max_depth=9)\n",
        "  model2.fit(x, residuals1)\n",
        "  output = model1.predict(xnew) + model2.predict(xnew)\n",
        "  return output"
      ],
      "metadata": {
        "id": "DuuCx-w-aRBx"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Doing a quick test of the boosted on the concrete dataset and seeing if varying the regressors changes the accuracy."
      ],
      "metadata": {
        "id": "WLJhX-2odThi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#I use this structure throughout the project.\n",
        "x = concrete.loc[:,'cement':'age'].values\n",
        "y = concrete['strength'].values\n",
        "\n",
        "xtrain, xtest, ytrain, ytest = tts(x,y,test_size=0.3,shuffle=True,random_state=123) #Creating test and train data.\n",
        "#Scaling Data\n",
        "xtrain = scale.fit_transform(xtrain)\n",
        "xtest = scale.transform(xtest)\n",
        "\n",
        "#Testing the boosted function with identical regressors\n",
        "yhat = boosted_lwr(xtrain, ytrain, xtest, f1=25/len(xtrain), f2=25/len(xtrain), iter1=1, iter2=1)\n",
        "\n",
        "mse(ytest,yhat) #To test accuracy."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xHojWogdRLM",
        "outputId": "e9b14c75-2bf7-4ea5-fab6-be6c66ed6884"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "57.73151261936397"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = concrete.loc[:,'cement':'age'].values\n",
        "y = concrete['strength'].values\n",
        "\n",
        "xtrain, xtest, ytrain, ytest = tts(x,y,test_size=0.3,shuffle=True,random_state=123) #Creating test and train data.\n",
        "#Scaling Data\n",
        "xtrain = scale.fit_transform(xtrain)\n",
        "xtest = scale.transform(xtest)\n",
        "\n",
        "#Testing the boosted function with different regressors\n",
        "yhat = boosted_lwr(xtrain, ytrain, xtest, f1=25/len(xtrain), f2=343/len(xtrain), iter1=1, iter2=3)\n",
        "\n",
        "mse(ytest,yhat) #To test accuracy."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3A7KB8Kex1m",
        "outputId": "1db15434-b09b-4e5a-d4dc-8dc2ef81f071"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "59.80109374389222"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using different regressors seem to hurt accuracy."
      ],
      "metadata": {
        "id": "8eH1bS-cfgLK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Question 2"
      ],
      "metadata": {
        "id": "I0rJlBaSC-Bp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the Boosted Locally Weighted Regressor with different choices of data (such as \"cars.csv\", \"concrete.csv\" and \"housing.csv\") and different choice of kernels, such as Gaussian, Tricubic, Epanechnikov and Quartic.\n",
        "\n",
        "Quartic seems to be the best performing kernel across all datasets."
      ],
      "metadata": {
        "id": "94KU2hQlDGnE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Kernels as coded in class with my descriptions.\n",
        "\n",
        "def Gaussian(w):\n",
        "  #Gaussian kernels are flatter in comparison to the other kernels and don't peak nearly as high.\n",
        "  #They also don't hit 0 as the inputs approach 1 to -1.\n",
        "  return np.where(w>4,0,1/(np.sqrt(2*np.pi))*np.exp(-1/2*w**2))\n",
        "\n",
        "def Tricubic(w):\n",
        "  #Tricubic kernels output flattens to 0 as the input approaches -1 and 1.\n",
        "  #Also flattens to 70/81 for output as input is near 0.\n",
        "  return np.where(w>1,0,70/81*(1-w**3)**3)\n",
        "\n",
        "def Quartic(w):\n",
        "  #Quartic kernels output slightly flattens to 0 as the input approaches -1 and 1.\n",
        "  #Looks like a negative quadratic graph.\n",
        "  return np.where(w>1,0,15/16*(1-w**2)**2)\n",
        "\n",
        "def Epanechnikov(w):\n",
        "  #Epanechnikov doesn't flatten out to 0 but just the curve continues to 0 as the input approaches -1 and 1.\n",
        "  #Not as high peak as tricubic and quartic kernels.\n",
        "  return np.where(w>1,0,3/4*(1-w**2))"
      ],
      "metadata": {
        "id": "Lq-_pJIVNPS8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Cars Dataset"
      ],
      "metadata": {
        "id": "i7x0vtr6gFvN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the cars dataset it seems that Quartic kernel seems to do the best followed by Gaussian, Tricubic, and finally Epanechnikov."
      ],
      "metadata": {
        "id": "ljMYvzJZjWPt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = cars.loc[:,'CYL':'WGT'].values\n",
        "y = cars['MPG'].values\n",
        "\n",
        "xtrain, xtest, ytrain, ytest = tts(x,y,test_size=0.3,shuffle=True,random_state=123) #Creating test and train data.\n",
        "#Scaling Data\n",
        "xtrain = scale.fit_transform(xtrain)\n",
        "xtest = scale.transform(xtest)\n",
        "\n",
        "#Testing the boosted function on the cars dataset using a Gaussian kernel.\n",
        "yhat = boosted_lwr(xtrain, ytrain, xtest, f1=1/3, f2=1/3, iter1=1, iter2=1, kernel1='Gaussian', kernel2='Gaussian')\n",
        "\n",
        "mse(ytest,yhat) #To test accuracy."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2Ur6ZwdC-9r",
        "outputId": "43b03a50-1a3f-4686-fff4-e97eb206e807"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17.171564080962355"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = cars.loc[:,'CYL':'WGT'].values\n",
        "y = cars['MPG'].values\n",
        "\n",
        "xtrain, xtest, ytrain, ytest = tts(x,y,test_size=0.3,shuffle=True,random_state=123) #Creating test and train data.\n",
        "#Scaling Data\n",
        "xtrain = scale.fit_transform(xtrain)\n",
        "xtest = scale.transform(xtest)\n",
        "\n",
        "#Testing the boosted function on the cars dataset using a Tricubic kernel.\n",
        "yhat = boosted_lwr(xtrain, ytrain, xtest, f1=1/3, f2=1/3, iter1=1, iter2=1, kernel1='Tricubic', kernel2='Tricubic')\n",
        "\n",
        "mse(ytest,yhat) #To test accuracy."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5FCqFMCUi6dS",
        "outputId": "5d77ac32-2de7-4d84-c479-bb62242a20dd"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17.19057553846594"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = cars.loc[:,'CYL':'WGT'].values\n",
        "y = cars['MPG'].values\n",
        "\n",
        "xtrain, xtest, ytrain, ytest = tts(x,y,test_size=0.3,shuffle=True,random_state=123) #Creating test and train data.\n",
        "#Scaling Data\n",
        "xtrain = scale.fit_transform(xtrain)\n",
        "xtest = scale.transform(xtest)\n",
        "\n",
        "#Testing the boosted function on the cars dataset using a Epanechnikov kernel.\n",
        "yhat = boosted_lwr(xtrain, ytrain, xtest, f1=1/3, f2=1/3, iter1=1, iter2=1, kernel1='Epanechnikov', kernel2='Epanechnikov')\n",
        "\n",
        "mse(ytest,yhat) #To test accuracy."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNRbGZ-ZjAEx",
        "outputId": "01f72804-394f-47f3-f5f1-2686e5db6712"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17.49105150231541"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = cars.loc[:,'CYL':'WGT'].values\n",
        "y = cars['MPG'].values\n",
        "\n",
        "xtrain, xtest, ytrain, ytest = tts(x,y,test_size=0.3,shuffle=True,random_state=123) #Creating test and train data.\n",
        "#Scaling Data\n",
        "xtrain = scale.fit_transform(xtrain)\n",
        "xtest = scale.transform(xtest)\n",
        "\n",
        "#Testing the boosted function on the cars dataset using a Quartic kernel.\n",
        "yhat = boosted_lwr(xtrain, ytrain, xtest, f1=1/3, f2=1/3, iter1=1, iter2=1, kernel1='Quartic', kernel2='Quartic')\n",
        "\n",
        "mse(ytest,yhat) #To test accuracy."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zqM28gajPWO",
        "outputId": "66196604-0696-43e9-e63e-e87389a8cf51"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17.13904731796445"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Concrete Dataset"
      ],
      "metadata": {
        "id": "rYQO8BUvgP0w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the concrete dataset it seems that Quartic kernel seems to do the best followed by Epanechnikov, Tricubic, and finally Gaussian."
      ],
      "metadata": {
        "id": "hEHNpjGWhNj4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = concrete.loc[:,'cement':'age'].values\n",
        "y = concrete['strength'].values\n",
        "\n",
        "xtrain, xtest, ytrain, ytest = tts(x,y,test_size=0.3,shuffle=True,random_state=123) #Creating test and train data.\n",
        "#Scaling Data\n",
        "xtrain = scale.fit_transform(xtrain)\n",
        "xtest = scale.transform(xtest)\n",
        "\n",
        "#Testing the boosted function on the concrete dataset using a Gaussian kernel.\n",
        "yhat = boosted_lwr(xtrain, ytrain, xtest, f1=25/len(xtrain), f2=25/len(xtrain), iter1=1, iter2=1, kernel1='Gaussian', kernel2='Gaussian')\n",
        "\n",
        "mse(ytest,yhat) #To test accuracy."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dt8xNH_gaeS",
        "outputId": "bdc79f44-5678-4868-e92a-95c426b7a31b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "110.19334505529065"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = concrete.loc[:,'cement':'age'].values\n",
        "y = concrete['strength'].values\n",
        "\n",
        "xtrain, xtest, ytrain, ytest = tts(x,y,test_size=0.3,shuffle=True,random_state=123) #Creating test and train data.\n",
        "#Scaling Data\n",
        "xtrain = scale.fit_transform(xtrain)\n",
        "xtest = scale.transform(xtest)\n",
        "\n",
        "#Testing the boosted function on the concrete dataset using a Tricubic kernel.\n",
        "yhat = boosted_lwr(xtrain, ytrain, xtest, f1=25/len(xtrain), f2=25/len(xtrain), iter1=1, iter2=1, kernel1='Tricubic', kernel2='Tricubic')\n",
        "\n",
        "mse(ytest,yhat) #To test accuracy."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0t63ZZWjg2Gu",
        "outputId": "1d1f7ce5-9c9c-447d-b7ed-ea698a4c5452"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "58.071638124882085"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = concrete.loc[:,'cement':'age'].values\n",
        "y = concrete['strength'].values\n",
        "\n",
        "xtrain, xtest, ytrain, ytest = tts(x,y,test_size=0.3,shuffle=True,random_state=123) #Creating test and train data.\n",
        "#Scaling Data\n",
        "xtrain = scale.fit_transform(xtrain)\n",
        "xtest = scale.transform(xtest)\n",
        "\n",
        "#Testing the boosted function on the concrete dataset using a Epanechnikov kernel.\n",
        "yhat = boosted_lwr(xtrain, ytrain, xtest, f1=25/len(xtrain), f2=25/len(xtrain), iter1=1, iter2=1, kernel1='Epanechnikov', kernel2='Epanechnikov')\n",
        "\n",
        "mse(ytest,yhat) #To test accuracy."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ajp6PzEhfhc",
        "outputId": "06b79476-479f-411a-91f8-290149e0df61"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "57.73151261936397"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = concrete.loc[:,'cement':'age'].values\n",
        "y = concrete['strength'].values\n",
        "\n",
        "xtrain, xtest, ytrain, ytest = tts(x,y,test_size=0.3,shuffle=True,random_state=123) #Creating test and train data.\n",
        "#Scaling Data\n",
        "xtrain = scale.fit_transform(xtrain)\n",
        "xtest = scale.transform(xtest)\n",
        "\n",
        "#Testing the boosted function on the concrete dataset using a Quartic kernel.\n",
        "yhat = boosted_lwr(xtrain, ytrain, xtest, f1=25/len(xtrain), f2=25/len(xtrain), iter1=1, iter2=1, kernel1='Quartic', kernel2='Quartic')\n",
        "\n",
        "mse(ytest,yhat) #To test accuracy."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDPZjrHchztQ",
        "outputId": "76957fef-a303-4be8-a4f5-933633224188"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "57.609741816889"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Housing Dataset"
      ],
      "metadata": {
        "id": "mAQhvLkKj4N5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the housing dataset it seems that Quartic kernel seems to do the best followed by Tricubic, Epanechnikov, and finally Gaussian."
      ],
      "metadata": {
        "id": "oxcEtZAFlQdc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = housing.drop('river',axis=1).loc[:,'crime':'lstat'].values\n",
        "y = housing['cmedv'].values\n",
        "\n",
        "xtrain, xtest, ytrain, ytest = tts(x,y,test_size=0.3,shuffle=True,random_state=123) #Creating test and train data.\n",
        "#Scaling Data\n",
        "xtrain = scale.fit_transform(xtrain)\n",
        "xtest = scale.transform(xtest)\n",
        "\n",
        "#Testing the boosted function on the housing dataset using a Gaussian kernel.\n",
        "yhat = boosted_lwr(xtrain, ytrain, xtest, f1=1/3, f2=1/3, iter1=1, iter2=1, kernel1='Gaussian', kernel2='Gaussian')\n",
        "\n",
        "mse(ytest,yhat) #To test accuracy."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FvI2bNOkPTf",
        "outputId": "b2391e88-2113-4a00-a6a6-7e91e7e501b8"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "29.700252735630144"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = housing.drop('river',axis=1).loc[:,'crime':'lstat'].values\n",
        "y = housing['cmedv'].values\n",
        "\n",
        "xtrain, xtest, ytrain, ytest = tts(x,y,test_size=0.3,shuffle=True,random_state=123) #Creating test and train data.\n",
        "#Scaling Data\n",
        "xtrain = scale.fit_transform(xtrain)\n",
        "xtest = scale.transform(xtest)\n",
        "\n",
        "#Testing the boosted function on the housing dataset using a Tricubic kernel.\n",
        "yhat = boosted_lwr(xtrain, ytrain, xtest, f1=1/3, f2=1/3, iter1=1, iter2=1, kernel1='Tricubic', kernel2='Tricubic')\n",
        "\n",
        "mse(ytest,yhat) #To test accuracy."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZZkf53Lksaz",
        "outputId": "aef2b961-ba34-41a3-b204-dd2d7c04f74d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19.775496874475312"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = housing.drop('river',axis=1).loc[:,'crime':'lstat'].values\n",
        "y = housing['cmedv'].values\n",
        "\n",
        "xtrain, xtest, ytrain, ytest = tts(x,y,test_size=0.3,shuffle=True,random_state=123) #Creating test and train data.\n",
        "#Scaling Data\n",
        "xtrain = scale.fit_transform(xtrain)\n",
        "xtest = scale.transform(xtest)\n",
        "\n",
        "#Testing the boosted function on the housing dataset using a Epanechnikov kernel.\n",
        "yhat = boosted_lwr(xtrain, ytrain, xtest, f1=1/3, f2=1/3, iter1=1, iter2=1, kernel1='Epanechnikov', kernel2='Epanechnikov')\n",
        "\n",
        "mse(ytest,yhat) #To test accuracy."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0IXnYQIok20O",
        "outputId": "fc9aa321-bac7-4144-8e85-0909071b207c"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20.222983884293082"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = housing.drop('river',axis=1).loc[:,'crime':'lstat'].values\n",
        "y = housing['cmedv'].values\n",
        "\n",
        "xtrain, xtest, ytrain, ytest = tts(x,y,test_size=0.3,shuffle=True,random_state=123) #Creating test and train data.\n",
        "#Scaling Data\n",
        "xtrain = scale.fit_transform(xtrain)\n",
        "xtest = scale.transform(xtest)\n",
        "\n",
        "#Testing the boosted function on the housing dataset using a Quartic kernel.\n",
        "yhat = boosted_lwr(xtrain, ytrain, xtest, f1=1/3, f2=1/3, iter1=1, iter2=1, kernel1='Quartic', kernel2='Quartic')\n",
        "\n",
        "mse(ytest,yhat) #To test accuracy."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnEpz6u9lEND",
        "outputId": "88ebc71a-cc76-40a8-a187-9a29506863b6"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19.45746532573222"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Question 3"
      ],
      "metadata": {
        "id": "Fi92YCmgDq8y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the complete K-Fold crossvalidations to compare with other regressors, such as RandomForest."
      ],
      "metadata": {
        "id": "J182G5bWDxsf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing using cars dataset.\n",
        "\n",
        "x = cars.loc[:,'CYL':'WGT'].values\n",
        "y = cars['MPG'].values\n",
        "\n",
        "mse_lwr = []\n",
        "mse_rf = []\n",
        "kf = KFold(n_splits=10,shuffle=True,random_state=1234)\n",
        "model_rf = RandomForestRegressor(n_estimators=200,max_depth=5)\n",
        "\n",
        "#Test Train Split\n",
        "for idxtrain, idxtest in kf.split(x):\n",
        "  xtrain = x[idxtrain]\n",
        "  ytrain = y[idxtrain]\n",
        "  ytest = y[idxtest]\n",
        "  xtest = x[idxtest]\n",
        "  xtrain = scale.fit_transform(xtrain)\n",
        "  xtest = scale.transform(xtest)\n",
        "\n",
        "  #Locally Weighted Regression\n",
        "  yhat_lw = boosted_lwr(xtrain, ytrain, xtest, f1=1/3, f2=1/3, iter1=1, iter2=1, kernel1='Quartic', kernel2='Quartic')\n",
        "  \n",
        "  #Random Forest\n",
        "  model_rf.fit(xtrain,ytrain)\n",
        "  yhat_rf = model_rf.predict(xtest)\n",
        "\n",
        "  mse_lwr.append(mse(ytest,yhat_lw)) #To test accuracy.\n",
        "  mse_rf.append(mse(ytest,yhat_rf)) #To test accuracy.\n",
        "print('The Cross-validated Mean Squared Error for Locally Weighted Regression using a Quartic kernel is : '+str(np.mean(mse_lwr)))\n",
        "print('The Cross-validated Mean Squared Error for Random Forest is : '+str(np.mean(mse_rf)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqdnWkvXDr2K",
        "outputId": "013acb7c-db58-4583-8868-34a53781795c"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Cross-validated Mean Squared Error for Locally Weighted Regression using a Quartic kernel is : 15.99975674599304\n",
            "The Cross-validated Mean Squared Error for Random Forest is : 17.17940703572381\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Locally Weighted Regression seems to be more accurate then the Random Forest for the cars dataset."
      ],
      "metadata": {
        "id": "KrWJCvyqo0uE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing using housing dataset.\n",
        "\n",
        "x = housing.drop('river',axis=1).loc[:,'crime':'lstat'].values\n",
        "y = housing['cmedv'].values\n",
        "\n",
        "mse_lwr = []\n",
        "mse_rf = []\n",
        "kf = KFold(n_splits=10,shuffle=True,random_state=1234)\n",
        "model_rf = RandomForestRegressor(n_estimators=200,max_depth=5)\n",
        "\n",
        "#Test Train Split\n",
        "for idxtrain, idxtest in kf.split(x):\n",
        "  xtrain = x[idxtrain]\n",
        "  ytrain = y[idxtrain]\n",
        "  ytest = y[idxtest]\n",
        "  xtest = x[idxtest]\n",
        "  xtrain = scale.fit_transform(xtrain)\n",
        "  xtest = scale.transform(xtest)\n",
        "\n",
        "  #Locally Weighted Regression\n",
        "  yhat_lw = boosted_lwr(xtrain, ytrain, xtest, f1=1/3, f2=1/3, iter1=1, iter2=1, kernel1='Quartic', kernel2='Quartic')\n",
        "  \n",
        "  #Random Forest\n",
        "  model_rf.fit(xtrain,ytrain)\n",
        "  yhat_rf = model_rf.predict(xtest)\n",
        "\n",
        "  mse_lwr.append(mse(ytest,yhat_lw)) #To test accuracy.\n",
        "  mse_rf.append(mse(ytest,yhat_rf)) #To test accuracy.\n",
        "print('The Cross-validated Mean Squared Error for Locally Weighted Regression using a Quartic kernel is : '+str(np.mean(mse_lwr)))\n",
        "print('The Cross-validated Mean Squared Error for Random Forest is : '+str(np.mean(mse_rf)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewPSwWvfpPkm",
        "outputId": "6632fa13-face-483a-b85e-df7b07e9bffa"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Cross-validated Mean Squared Error for Locally Weighted Regression using a Quartic kernel is : 17.666857991099906\n",
            "The Cross-validated Mean Squared Error for Random Forest is : 14.244387713106562\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Locally Weighted Regression seems to be less accurate then the Random Forest for the housing dataset."
      ],
      "metadata": {
        "id": "-krmKjBpqFAp"
      }
    }
  ]
}